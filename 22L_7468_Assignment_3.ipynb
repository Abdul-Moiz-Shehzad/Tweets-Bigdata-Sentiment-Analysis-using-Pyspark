{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6c45deaf-bfc5-45c2-aca2-77da17b16431",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c45deaf-bfc5-45c2-aca2-77da17b16431",
        "outputId": "146c0f68-a1b4-4f02-f6f3-c28a02f3ae26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=2b1cdb2003f84379218e7c7a0b4a4729a0f0443c8ff4a2ac65604505a0e6e5ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c2918778-aea3-4ba3-a556-efb4d13c2447",
      "metadata": {
        "id": "c2918778-aea3-4ba3-a556-efb4d13c2447"
      },
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "85641ec0-b31c-4161-b256-a0bfc2232ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "85641ec0-b31c-4161-b256-a0bfc2232ce2",
        "outputId": "084d4a3f-89c9-422f-8e36-77b5a2972247"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c13d0f5c405c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Assginment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78cbdf63a380>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Assginment').getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7eddf72b-ecbd-42c8-becb-37a28d360957",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eddf72b-ecbd-42c8-becb-37a28d360957",
        "outputId": "581ddda6-d823-45e8-bbf6-a3fbad1cf38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+--------+---------+\n",
            "|  tweet_date_created|tweet_id|          tweet_text|language|sentiment|\n",
            "+--------------------+--------+--------------------+--------+---------+\n",
            "|2018-06-30T19:26:...|1.01E+18|I vote #BernardoC...|      en|  NEUTRAL|\n",
            "|2018-08-12T14:23:...|1.03E+18|When is your firs...|      en|  NEUTRAL|\n",
            "|2018-07-17T10:50:...|1.02E+18|#Cristiano You ne...|      en|  NEUTRAL|\n",
            "| 2018-06-08T15:20:06|1.01E+18|#youngy18 #Englan...|      en| POSITIVE|\n",
            "|2018-07-28T17:25:...|1.02E+18|#LFC #officialAL2...|      en|  NEUTRAL|\n",
            "+--------------------+--------+--------------------+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.csv('hdfs://localhost:9000/data/TweetsDataset.csv',header=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a95c515e-cae3-4c2b-9431-e002bcd197d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95c515e-cae3-4c2b-9431-e002bcd197d6",
        "outputId": "4aa62259-7a97-47d7-c42e-6222ea899ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicates= 1023944\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of duplicates=\",df.count()-df.drop_duplicates().count())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e197d4-5007-4b1a-9c49-c8e263988628",
      "metadata": {
        "id": "19e197d4-5007-4b1a-9c49-c8e263988628"
      },
      "source": [
        "Drop the duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1fb915b2-315a-4514-8ef7-2486fab3ed0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fb915b2-315a-4514-8ef7-2486fab3ed0c",
        "outputId": "3b11f808-637c-4b22-ff36-0751da28bf09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6592620\n"
          ]
        }
      ],
      "source": [
        "df=df.drop_duplicates()\n",
        "count=df.count()\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4140430c-fa1b-46b1-a1ea-dbf474c568ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4140430c-fa1b-46b1-a1ea-dbf474c568ff",
        "outputId": "474c4b21-699b-4cb7-9ba0-94b04190f2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+--------------------+---------+\n",
            "|  tweet_date_created|tweet_id|          tweet_text|sentiment|\n",
            "+--------------------+--------+--------------------+---------+\n",
            "|2018-07-20T19:46:...|1.02E+18|#leeabellis #andr...|  NEUTRAL|\n",
            "|2018-09-12T20:32:...|1.04E+18|#Paschal_MUFC #LF...|  NEUTRAL|\n",
            "| 2018-06-02T12:59:17|1.00E+18|#jnoahmorgan #hen...|  NEUTRAL|\n",
            "|2018-07-24T08:30:...|1.02E+18|#HenrikhMkh #Auba...| POSITIVE|\n",
            "|2018-07-10T22:47:...|1.02E+18|#Howmanyroads1 #b...|  NEUTRAL|\n",
            "+--------------------+--------+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=df.drop('language')\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b8f72e-c119-4cc7-a8b6-738fd45a3f9c",
      "metadata": {
        "id": "00b8f72e-c119-4cc7-a8b6-738fd45a3f9c"
      },
      "source": [
        "Clean date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9184d19f-68ff-4371-aef3-91a8062345fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9184d19f-68ff-4371-aef3-91a8062345fd",
        "outputId": "89f137cc-d420-4434-ced6-efaf1d42ba10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- tweet_date_created: string (nullable = true)\n",
            " |-- tweet_id: string (nullable = true)\n",
            " |-- tweet_text: string (nullable = true)\n",
            " |-- sentiment: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf,expr,col\n",
        "df=df.withColumn(\"tweet_date_created\",col('tweet_date_created').cast('string'))\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "yTcyCrapsjqe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTcyCrapsjqe",
        "outputId": "6274ef42-7093-450e-dd92-f6a1b1bd5b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+--------+--------------------+---------+\n",
            "|tweet_date_created|tweet_id|          tweet_text|sentiment|\n",
            "+------------------+--------+--------------------+---------+\n",
            "|        2018-07-20|1.02E+18|#leeabellis #andr...|  NEUTRAL|\n",
            "|        2018-09-12|1.04E+18|#Paschal_MUFC #LF...|  NEUTRAL|\n",
            "|        2018-06-02|1.00E+18|#jnoahmorgan #hen...|  NEUTRAL|\n",
            "|        2018-07-24|1.02E+18|#HenrikhMkh #Auba...| POSITIVE|\n",
            "|        2018-07-10|1.02E+18|#Howmanyroads1 #b...|  NEUTRAL|\n",
            "+------------------+--------+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def cleanDate(date):\n",
        "  date = date.split(\"T\")[0]\n",
        "  return (date)\n",
        "cleanDateFunction=udf(lambda x:cleanDate(x),StringType())\n",
        "df=df.withColumn(\"tweet_date_created\",cleanDateFunction(df.tweet_date_created))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d9ef57-28e1-462e-bed8-6e5c6a98ab04",
      "metadata": {
        "id": "73d9ef57-28e1-462e-bed8-6e5c6a98ab04"
      },
      "source": [
        "Remove retweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "biitO-N-w5d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biitO-N-w5d8",
        "outputId": "f60a2940-e051-4f21-c0bf-318aa2513656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before= 6592620\n"
          ]
        }
      ],
      "source": [
        "print(\"Before=\",count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "27d40d44-89d0-45c4-b85d-fe73377638cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27d40d44-89d0-45c4-b85d-fe73377638cd",
        "outputId": "f7ba9b53-f405-41b5-8c73-ea65514a45de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+--------+--------------------+---------+\n",
            "|tweet_date_created|tweet_id|          tweet_text|sentiment|\n",
            "+------------------+--------+--------------------+---------+\n",
            "|        2018-07-20|1.02E+18|#leeabellis #andr...|  NEUTRAL|\n",
            "|        2018-09-12|1.04E+18|#Paschal_MUFC #LF...|  NEUTRAL|\n",
            "|        2018-06-02|1.00E+18|#jnoahmorgan #hen...|  NEUTRAL|\n",
            "|        2018-07-24|1.02E+18|#HenrikhMkh #Auba...| POSITIVE|\n",
            "|        2018-07-10|1.02E+18|#Howmanyroads1 #b...|  NEUTRAL|\n",
            "+------------------+--------+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=df.filter(~df.tweet_text.contains('Retweeted'))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d5a0b601-1bfd-4753-8c30-8a7804659f22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5a0b601-1bfd-4753-8c30-8a7804659f22",
        "outputId": "64373a83-168c-4e2c-8af2-da9863881e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After= 6590377\n"
          ]
        }
      ],
      "source": [
        "count=df.count()\n",
        "print(\"After=\",count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mVfptxmgx2Gj",
      "metadata": {
        "id": "mVfptxmgx2Gj"
      },
      "source": [
        "Remove irrelevent tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "80007cda-86cd-4d57-b31c-f0c16807a469",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80007cda-86cd-4d57-b31c-f0c16807a469",
        "outputId": "d14371ab-c076-4d58-c568-facbe283eea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6234684\n"
          ]
        }
      ],
      "source": [
        "df=df.filter(\"Sentiment like '%POSITIVE%' or Sentiment like '%NEGATIVE%' or Sentiment like '%NEUTRAL%'\")\n",
        "df=df.filter(~df.tweet_text.contains('http'))\n",
        "count=df.count()\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ap-RO9Yyjok",
      "metadata": {
        "id": "9ap-RO9Yyjok"
      },
      "source": [
        "Realized you dont need tweet_Id, tweet_date as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f6153cd4-6290-44a0-b2e6-3899bd0b65d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6153cd4-6290-44a0-b2e6-3899bd0b65d4",
        "outputId": "ee1d811d-7fcf-4e86-d22a-34bc14400f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+\n",
            "|          tweet_text|sentiment|\n",
            "+--------------------+---------+\n",
            "|#leeabellis #andr...|  NEUTRAL|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|\n",
            "+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=df.drop('tweet_date_created','tweet_id')\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y6ZLSQM6zDYk",
      "metadata": {
        "id": "Y6ZLSQM6zDYk"
      },
      "source": [
        "Removing Stop words and tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "06Yadcxly4bK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Yadcxly4bK",
        "outputId": "e93a3937-8b88-44f7-d837-eb1d9ab3ed0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+\n",
            "|          tweet_text|sentiment|       SplittedWords|\n",
            "+--------------------+---------+--------------------+\n",
            "|#leeabellis #andr...|  NEUTRAL|[How, is, he, a, ...|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|[How, did, you, m...|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|[I, just, apologi...|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|[Good, luck, this...|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|[I, believe, he, ...|\n",
            "|Every starter for...|  NEUTRAL|[Every, starter, ...|\n",
            "|#chazaustin10 #CA...|  NEUTRAL|[Charkie10, ?...,...|\n",
            "|#D_DeGea #BumperG...|  NEUTRAL|[Come, on, Baxter...|\n",
            "|#MarkMoraghan #18...|  NEUTRAL|[I, will, fight, ...|\n",
            "|#andresiniesta8 M...|  NEUTRAL|[Muchas, gracias,...|\n",
            "|#GoloFutbol #MrDt...|  NEUTRAL|[At, least, Lacaz...|\n",
            "|#PaulMJ875 #Cam_0...|  NEUTRAL|[considering, the...|\n",
            "|#MarcusRashford #...| POSITIVE|[This, gave, me, ...|\n",
            "|#SpursOfficial #A...|  NEUTRAL|[Hugo, certainly,...|\n",
            "|#sheffraf #jamesd...|  NEUTRAL|[England, are, th...|\n",
            "|China Goods #paul...| POSITIVE|[China, Goods, go...|\n",
            "|#mjmanojmanoj #Ez...|  NEUTRAL|             [Thank]|\n",
            "|#RamyCol #MoSalah...| POSITIVE|[happy, birthday,...|\n",
            "|#JPickford1 #Mich...|  NEUTRAL|[Nandoski, mate.,...|\n",
            "|#NiallOfficial #n...|  NEUTRAL|[i, finally, unde...|\n",
            "+--------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+---------+--------------------+--------------------+\n",
            "|          tweet_text|sentiment|       SplittedWords|    SplittedWordsNew|\n",
            "+--------------------+---------+--------------------+--------------------+\n",
            "|#leeabellis #andr...|  NEUTRAL|[How, is, he, a, ...|[big, loss?, bare...|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|[How, did, you, m...|[mean, country, b...|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|[I, just, apologi...|[apologize, ?., I...|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|[Good, luck, this...|[Good, luck, seas...|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|[I, believe, he, ...|[believe, verball...|\n",
            "|Every starter for...|  NEUTRAL|[Every, starter, ...|[Every, starter, ...|\n",
            "|#chazaustin10 #CA...|  NEUTRAL|[Charkie10, ?...,...|[Charkie10, ?...,...|\n",
            "|#D_DeGea #BumperG...|  NEUTRAL|[Come, on, Baxter...|[Come, Baxter, kn...|\n",
            "|#MarkMoraghan #18...|  NEUTRAL|[I, will, fight, ...|    [fight, ya, lad]|\n",
            "|#andresiniesta8 M...|  NEUTRAL|[Muchas, gracias,...|[Muchas, gracias,...|\n",
            "|#GoloFutbol #MrDt...|  NEUTRAL|[At, least, Lacaz...|[least, Lacazette...|\n",
            "|#PaulMJ875 #Cam_0...|  NEUTRAL|[considering, the...|[considering, amo...|\n",
            "|#MarcusRashford #...| POSITIVE|[This, gave, me, ...|[gave, chills, co...|\n",
            "|#SpursOfficial #A...|  NEUTRAL|[Hugo, certainly,...|[Hugo, certainly,...|\n",
            "|#sheffraf #jamesd...|  NEUTRAL|[England, are, th...|[England, first, ...|\n",
            "|China Goods #paul...| POSITIVE|[China, Goods, go...|[China, Goods, go...|\n",
            "|#mjmanojmanoj #Ez...|  NEUTRAL|             [Thank]|             [Thank]|\n",
            "|#RamyCol #MoSalah...| POSITIVE|[happy, birthday,...|[happy, birthday,...|\n",
            "|#JPickford1 #Mich...|  NEUTRAL|[Nandoski, mate.,...|[Nandoski, mate.,...|\n",
            "|#NiallOfficial #n...|  NEUTRAL|[i, finally, unde...|[finally, underst...|\n",
            "+--------------------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "def tokenizeText(text):\n",
        "  lst=[]\n",
        "  text=text.split(' ')\n",
        "  for words in text:\n",
        "    if words.find('@')==-1:\n",
        "      if words.find('#')==-1:\n",
        "        lst.append(words)\n",
        "  return lst\n",
        "tokenizeTextFunction=udf(lambda x: tokenizeText(x),ArrayType(StringType()))\n",
        "df=df.withColumn(\"SplittedWords\",tokenizeTextFunction(df.tweet_text))\n",
        "df.show()\n",
        "\n",
        "stopwordsremover=StopWordsRemover(inputCol=\"SplittedWords\",outputCol=\"SplittedWordsNew\")\n",
        "df=stopwordsremover.transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7jvJn_Jw4MQk",
      "metadata": {
        "id": "7jvJn_Jw4MQk"
      },
      "outputs": [],
      "source": [
        "df=df.drop('SplittedWords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53UJ3p1k61s5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53UJ3p1k61s5",
        "outputId": "c22b01bc-beae-4958-f6eb-5b26811e2f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rND8uV1V7vcL",
      "metadata": {
        "id": "rND8uV1V7vcL"
      },
      "source": [
        "stemming and lemmatization of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Ffvpq1CJ7XI-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffvpq1CJ7XI-",
        "outputId": "513f3745-224e-4e92-ed86-697251cdccc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "DT2_Dli75Eod",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT2_Dli75Eod",
        "outputId": "76e7bcf1-fb56-4c60-9bd2-e2f79af2b207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "|          tweet_text|sentiment|    SplittedWordsNew|       stemmed_words|    lemmatized_words|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "|#leeabellis #andr...|  NEUTRAL|[big, loss?, bare...|[big, loss?, bare...|[big, loss?, bare...|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|[mean, country, b...|[mean, countri, b...|[mean, countri, b...|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|[apologize, ?., I...|[apolog, ?., i ll...|[apolog, ?., i ll...|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|[Good, luck, seas...|[good, luck, seas...|[good, luck, seas...|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|[believe, verball...|[believ, verbal, ...|[believ, verbal, ...|\n",
            "|Every starter for...|  NEUTRAL|[Every, starter, ...|[everi, starter, ...|[everi, starter, ...|\n",
            "|#chazaustin10 #CA...|  NEUTRAL|[Charkie10, ?...,...|[charkie10, ?...,...|[charkie10, ?...,...|\n",
            "|#D_DeGea #BumperG...|  NEUTRAL|[Come, Baxter, kn...|[come, baxter, kn...|[come, baxter, kn...|\n",
            "|#MarkMoraghan #18...|  NEUTRAL|    [fight, ya, lad]|    [fight, ya, lad]|    [fight, ya, lad]|\n",
            "|#andresiniesta8 M...|  NEUTRAL|[Muchas, gracias,...|[mucha, gracia, c...|[mucha, gracia, c...|\n",
            "|#GoloFutbol #MrDt...|  NEUTRAL|[least, Lacazette...|[least, lacazett,...|[least, lacazett,...|\n",
            "|#PaulMJ875 #Cam_0...|  NEUTRAL|[considering, amo...|[consid, amount, ...|[consid, amount, ...|\n",
            "|#MarcusRashford #...| POSITIVE|[gave, chills, co...|[gave, chill, com...|[gave, chill, com...|\n",
            "|#SpursOfficial #A...|  NEUTRAL|[Hugo, certainly,...|[hugo, certain, b...|[hugo, certain, b...|\n",
            "|#sheffraf #jamesd...|  NEUTRAL|[England, first, ...|[england, first, ...|[england, first, ...|\n",
            "|China Goods #paul...| POSITIVE|[China, Goods, go...|[china, good, goo...|[china, good, goo...|\n",
            "|#mjmanojmanoj #Ez...|  NEUTRAL|             [Thank]|             [thank]|             [thank]|\n",
            "|#RamyCol #MoSalah...| POSITIVE|[happy, birthday,...|[happi, birthday,...|[happi, birthday,...|\n",
            "|#JPickford1 #Mich...|  NEUTRAL|[Nandoski, mate.,...|[nandoski, mate.,...|[nandoski, mate.,...|\n",
            "|#NiallOfficial #n...|  NEUTRAL|[finally, underst...| [final, understand]| [final, understand]|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_words(words):\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "stemming_udf = udf(stem_words, ArrayType(StringType()))\n",
        "lemmatization_udf = udf(lemmatize_words, ArrayType(StringType()))\n",
        "\n",
        "df = df.withColumn('stemmed_words', stemming_udf('SplittedWordsNew'))\n",
        "df = df.withColumn('lemmatized_words', lemmatization_udf('stemmed_words'))\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GI5FtopPAsGX",
      "metadata": {
        "id": "GI5FtopPAsGX"
      },
      "source": [
        "Handling emojis (i forgot bout that earlier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9uLMQF7k-dlh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uLMQF7k-dlh",
        "outputId": "f5ef8eb0-4ec0-4051-fb95-23cc9ce09ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
            "|          tweet_text|sentiment|    SplittedWordsNew|       stemmed_words|    lemmatized_words|           no_emojis|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
            "|#leeabellis #andr...|  NEUTRAL|[big, loss?, bare...|[big, loss?, bare...|[big, loss?, bare...|[big, loss?, bare...|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|[mean, country, b...|[mean, countri, b...|[mean, countri, b...|[mean, countri, b...|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|[apologize, ?., I...|[apolog, ?., i ll...|[apolog, ?., i ll...|[apolog, ?., ill,...|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|[Good, luck, seas...|[good, luck, seas...|[good, luck, seas...|[good, luck, seas...|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|[believe, verball...|[believ, verbal, ...|[believ, verbal, ...|[believ, verbal, ...|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def removeEmojis(words):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return [emoji_pattern.sub(r'', word) for word in words]\n",
        "\n",
        "removeEmojisFunction = udf(removeEmojis, ArrayType(StringType()))\n",
        "\n",
        "df = df.withColumn('no_emojis', removeEmojisFunction(df.lemmatized_words))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HHNR_i-kEPQM",
      "metadata": {
        "id": "HHNR_i-kEPQM"
      },
      "source": [
        "#Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mJkvTXpxRFTX",
      "metadata": {
        "id": "mJkvTXpxRFTX"
      },
      "source": [
        "Using pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2X6MB5evEkiR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X6MB5evEkiR",
        "outputId": "865bbf24-1a2e-4486-d0da-275fd4ffa66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy= 0.7785016286644951\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "pandas_df=df.toPandas()\n",
        "\n",
        "pandas_df['no_emojis'] = pandas_df['no_emojis'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(pandas_df['no_emojis'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pandas_df['sentiment'], test_size=0.2, random_state=42)\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy=\",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OYlGuCnH5YfG",
      "metadata": {
        "id": "OYlGuCnH5YfG"
      },
      "source": [
        "Using pyspark ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "smPkqISP4R7r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPkqISP4R7r",
        "outputId": "ab76353e-2944-4e32-bae6-6c9ce78ebc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy= 0.7038461538461539\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "df=df.drop('features','label')\n",
        "\n",
        "indexer = StringIndexer(inputCol='sentiment', outputCol='label')\n",
        "df = indexer.fit(df).transform(df)\n",
        "vectorizer = CountVectorizer(inputCol='no_emojis', outputCol='features')\n",
        "model = vectorizer.fit(df)\n",
        "df = model.transform(df)\n",
        "\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(labelCol='label')\n",
        "lr_model = lr.fit(train)\n",
        "\n",
        "predictions = lr_model.transform(test)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"accuracy=\",accuracy)\n",
        "df=df.drop('features','label')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaLMdJzzRI2I",
      "metadata": {
        "id": "aaLMdJzzRI2I"
      },
      "source": [
        "Using nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "hntMmA40SGgU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hntMmA40SGgU",
        "outputId": "a59a4b33-8b70-471d-9e4d-18182d8dba9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
            "|          tweet_text|sentiment|    SplittedWordsNew|       stemmed_words|    lemmatized_words|           no_emojis|SentimentResult|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
            "|#leeabellis #andr...|  NEUTRAL|[big, loss?, bare...|[big, loss?, bare...|[big, loss?, bare...|[big, loss?, bare...|       positive|\n",
            "|#Paschal_MUFC #LF...|  NEUTRAL|[mean, country, b...|[mean, countri, b...|[mean, countri, b...|[mean, countri, b...|       negative|\n",
            "|#jnoahmorgan #hen...|  NEUTRAL|[apologize, ?., I...|[apolog, ?., i ll...|[apolog, ?., i ll...|[apolog, ?., ill,...|       negative|\n",
            "|#HenrikhMkh #Auba...| POSITIVE|[Good, luck, seas...|[good, luck, seas...|[good, luck, seas...|[good, luck, seas...|       positive|\n",
            "|#Howmanyroads1 #b...|  NEUTRAL|[believe, verball...|[believ, verbal, ...|[believ, verbal, ...|[believ, verbal, ...|       negative|\n",
            "|Every starter for...|  NEUTRAL|[Every, starter, ...|[everi, starter, ...|[everi, starter, ...|[everi, starter, ...|       positive|\n",
            "|#chazaustin10 #CA...|  NEUTRAL|[Charkie10, ?...,...|[charkie10, ?...,...|[charkie10, ?...,...|[charkie10, ?...,...|        neutral|\n",
            "|#D_DeGea #BumperG...|  NEUTRAL|[Come, Baxter, kn...|[come, baxter, kn...|[come, baxter, kn...|[come, baxter, kn...|        neutral|\n",
            "|#MarkMoraghan #18...|  NEUTRAL|    [fight, ya, lad]|    [fight, ya, lad]|    [fight, ya, lad]|    [fight, ya, lad]|       negative|\n",
            "|#andresiniesta8 M...|  NEUTRAL|[Muchas, gracias,...|[mucha, gracia, c...|[mucha, gracia, c...|[mucha, gracia, c...|        neutral|\n",
            "|#GoloFutbol #MrDt...|  NEUTRAL|[least, Lacazette...|[least, lacazett,...|[least, lacazett,...|[least, lacazett,...|       negative|\n",
            "|#PaulMJ875 #Cam_0...|  NEUTRAL|[considering, amo...|[consid, amount, ...|[consid, amount, ...|[consid, amount, ...|       negative|\n",
            "|#MarcusRashford #...| POSITIVE|[gave, chills, co...|[gave, chill, com...|[gave, chill, com...|[gave, chill, com...|        neutral|\n",
            "|#SpursOfficial #A...|  NEUTRAL|[Hugo, certainly,...|[hugo, certain, b...|[hugo, certain, b...|[hugo, certain, b...|       positive|\n",
            "|#sheffraf #jamesd...|  NEUTRAL|[England, first, ...|[england, first, ...|[england, first, ...|[england, first, ...|       negative|\n",
            "|China Goods #paul...| POSITIVE|[China, Goods, go...|[china, good, goo...|[china, good, goo...|[china, good, goo...|       positive|\n",
            "|#mjmanojmanoj #Ez...|  NEUTRAL|             [Thank]|             [thank]|             [thank]|             [thank]|       positive|\n",
            "|#RamyCol #MoSalah...| POSITIVE|[happy, birthday,...|[happi, birthday,...|[happi, birthday,...|[happi, birthday,...|       positive|\n",
            "|#JPickford1 #Mich...|  NEUTRAL|[Nandoski, mate.,...|[nandoski, mate.,...|[nandoski, mate.,...|[nandoski, mate.,...|        neutral|\n",
            "|#NiallOfficial #n...|  NEUTRAL|[finally, underst...| [final, understand]| [final, understand]| [final, understand]|        neutral|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def computeSentiment(words):\n",
        "    text = ' '.join(words)\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    if sentiment_scores['compound'] > 0:\n",
        "        return 'positive'\n",
        "    elif sentiment_scores['compound'] < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "computeSentimentFunction = udf(computeSentiment, StringType())\n",
        "df = df.withColumn('SentimentResult', computeSentimentFunction(df['no_emojis']))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "_OBeVGK_WCkc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OBeVGK_WCkc",
        "outputId": "fa40c900-2d58-44ec-a521-c005b25cbc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------+--------+-------+--------+\n",
            "|sentiment_SentimentResult|negative|neutral|positive|\n",
            "+-------------------------+--------+-------+--------+\n",
            "|                 POSITIVE|   50070| 100141|  518305|\n",
            "|                 NEGATIVE|  229538|  45001|   70457|\n",
            "|                  NEUTRAL|  393905|1945364|  242446|\n",
            "+-------------------------+--------+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "confusion_matrix = df.crosstab('sentiment', 'SentimentResult')\n",
        "confusion_matrix.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ee15b369-6dc1-4875-bdd0-eb7ea68eb319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee15b369-6dc1-4875-bdd0-eb7ea68eb319",
        "outputId": "4a305022-055e-427b-d4ba-b8be67f2ae8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+\n",
            "|Sentiment|   count|\n",
            "+---------+--------+\n",
            "| POSITIVE|  668516|\n",
            "| NEGATIVE|  264996|\n",
            "|  NEUTRAL| 2502936|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupby('Sentiment').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6c718d-337e-4cbe-a2c9-02b6cfefeacf",
      "metadata": {
        "id": "fe6c718d-337e-4cbe-a2c9-02b6cfefeacf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
